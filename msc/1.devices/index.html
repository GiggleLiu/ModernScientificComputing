<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/ModernScientificComputing/libs/katex/katex.min.css"> <link rel=stylesheet  href="/ModernScientificComputing/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/ModernScientificComputing/css/franklin.css"> <link rel=stylesheet  href="/ModernScientificComputing/css/basic.css"> <link rel=icon  href="/ModernScientificComputing/assets/favicon.jpg"> <title>1. Understanding our computing devices</title> <header> <div class=blog-name ><a href="/ModernScientificComputing/">Modern Scientific Computing</a></div> <nav> <ul> <li><a href="/ModernScientificComputing/">Home</a> </ul> <img src="/ModernScientificComputing/assets/msc2.jpg" id=menu-icon > </nav> </header> <div class=franklin-content ><h1 id=understanding_our_computing_devices_draft ><a href="#understanding_our_computing_devices_draft" class=header-anchor ><ol> <li><p>Understanding our computing devices &#40;draft&#41;</p> </ol> </a></h1> <p>Scientific computing is a combination of scientific applications, mathematical modeling and high performance computing. The first lecture focuses on understanding our computing devices and understand how to get high performance from them.</p> <h2 id=computer_architechture ><a href="#computer_architechture" class=header-anchor >Computer architechture</a></h2> <p>The <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">Von Neumann architecture</a> design consists of a Control Unit, Arithmetic and Logic Unit &#40;ALU&#41;, Memory Unit, Registers and Inputs/Outputs, while the control unit, ALU, and registers are all contained in a central processing unit &#40;CPU&#41;.</p> <p><a href="/ModernScientificComputing/assets/images/arch.png"><img src="/ModernScientificComputing/assets/images/arch.png" alt="" /></a></p> <p>As illustrated above &#40;click the image to make it larger&#41;, a modern computer contains the following building blocks</p> <ul> <li><p>A CPU</p> <ul> <li><p>A <strong>CPU clock</strong> signal is originally generated by quartz crystal oscillators of about 20MHz or so, and then the frequency is multiplied by one or more phase-locked loops to generate the clock signals for different parts of the system. &#40;such as 4GHz for a CPU core&#41;.</p> <li><p>A <strong>control unit</strong> is circuitry within a computer’s processor that directs operations. It instructs the memory, logic unit, and both output and input devices of the computer on how to respond to the program’s instructions. CPUs and GPUs are examples of devices that use control units.</p> <li><p><strong>Registers</strong> are the fastest &quot;storage&quot; shiped with CPU, directly accessible to the processor and works in the same clock rate as the CPU clock. The size of a general purposed register in a x64 machine is typically 64 bit while some wide registers designed for <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">Single Instruction, Multiple Data &#40;SIMD&#41;</a> can be several times larger, which allows processing multiple data points simultaneously with a single instruction. For example, if your computer supports the <code>avx2</code> instruction set, there must be a wide register of size 256 &#40;four 64-bit floating point numbers&#41;.</p> <li><p>An <a href="https://en.wikipedia.org/wiki/Arithmetic_logic_unit">ALU</a> is a combinational digital circuit that performs arithmetic and bitwise operations on integer binary numbers. </p> </ul> <li><p>The <a href="https://en.wikipedia.org/wiki/Memory_hierarchy">memory hierarchy</a> separates computer storage into a hierarchy based on response time. The fastest-smallest L1 cache works in the same speed as CPU, while the slowest-largest main memory has a ~50ns latency.</p> <li><p>A <a href="https://en.wikipedia.org/wiki/System_bus">system bus</a> is a single computer bus that connects the major components of a computer system, combining the functions of a data bus to carry information, an address bus to determine where it should be sent or read from, and a control bus to determine its operation.</p> <li><p>I/O devices are the pieces of hardware used by a human &#40;or other system&#41; to communicate with a computer. For instance, a keyboard or computer mouse is an input device for a computer, while monitors and printers are output devices.</p> </ul> <p>For some devices, it also contains</p> <ul> <li><p><a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">General-purpose computing on graphics processing units &#40;GPGPU&#41;</a> is the use of a graphics processing unit &#40;GPU&#41;, which typically handles computation only for computer graphics, to perform computation in applications traditionally handled by the central processing unit &#40;CPU&#41;. The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors &#40;SMs&#41;.</p> </ul> <h2 id=compiling_process ><a href="#compiling_process" class=header-anchor >Compiling process</a></h2> <p>A program is compiled to binaries &#40;e.g. 010011...&#41; in the main memory of a computer. The binary encodes both instructions and data, while a instruction can be an arithematic/logical operation to manipulate data in registers, transfering data from the main memory to registers and vise versa, or branching. For different computing devices, the supported instruction sets can be very different.</p> <p>To execute the binary, the computing device &#40;e.g. a CPU or a GPU&#41; first loads the instructions from the memory to the instruction register. After performing the arithmetic/logical instructions on the data in registers, the data in the result register is copied back to the main memory.</p> <p>In practise, a user program is executed in an operating system, which is a special software that manages the computing resources for other programs. It devides the computing time into small chunks of size ranging from milliseconds to tens of milliseconds and allocate them to different processes.</p> <h2 id=memory_bandwidth_and_latency ><a href="#memory_bandwidth_and_latency" class=header-anchor >Memory bandwidth and latency</a></h2> <p>Memory bandwidth is the speed of data transmission speed from the main memory to the registers, which is much slower than the speed of computing devices manipulating the data. To get the designed peak performance from your computing devices, you need to perform a certain number of operations on a data before deallocating it from the cache. The expected arithmetic indensity is defined as the ratio between the FLOPS and data rate, which is equal to the number of operations to be done on a data to equate the time spent on data transmission and manipulation. You may find a good introduction about the latency problem in this youtube clip.</p> <iframe width=560  height=315  src="https://www.youtube.com/embed/3l10o0DYJXg" title="YouTube video player" frameborder=0  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> <p>The following is a table from the above video,</p> <table><tr><th align=right ><th align=right >Intel Xeon 8280<th align=right >NVIDIA A100<tr><td align=right >Clock Cycle<td align=right >0.37ns<td align=right >0.71ns<tr><td align=right >Peak FP32 GFLOPS<td align=right >2190<td align=right >19500<tr><td align=right >Memory bandwidth &#40;GB/s&#41;<td align=right >131<td align=right >1555<tr><td align=right >DRAM Latency &#40;ns&#41;<td align=right >89<td align=right >404</table> <p><code>Peak FP32 GFLOPS</code> means the maximum number of single precision &#40;32 bit&#41; floating point operations per second, which is a standard measure of device performance. THe memory bandwidth is computed in unit of Giga-Byte per second, where <code>1 byte &#61; 8 bits</code>. DRAM is the dynamic random access memory, serving as the main memory.</p> <p>We can read from this table that typically</p> <ol> <li><p>a GPU works in a much lower clock speed that a CPU.</p> <li><p>a GPU has much larger computing power in the terms of FLOPS, because it has many CUDA cores &#40;stream processors&#41;.</p> <li><p>the bandwidth of a GPU is much larger than that of a CPU. Comparing with their computing power, the bandwidth is small in both cases. Only if the data in the cache can be reused for certain number of times, i.e. high arithematic intensity, the computing power of a device can be the actual bottleneck.</p> <li><p>The latency is several orders larger than the CPU/GPU clock time, which physically bottlenecked by the speed of light.</p> </ol> <p>One of the main differences between CPU and GPU architectures are due to their different approaches to solve the latency issue. CPUs are highly optimized for reducing the latency, i.e. to solve a task as soon as possible. Engineers designed a complicated 3-level caching system for CPUs that exists in the majority of our modern computers. Caches belongs to the static random access memory &#40;SRAM&#41;, which is much faster to access than the main memory or the dynamic random access memory &#40;DRAM&#41; we talk about in our daily life. As is illustrated in the figure, by the order of decreasing speed and increasing size, caches include fastest L1 cache working in the same speed as CPU, slower L2 cache, and the slowest L3 cache but still faster than the DRAM. When loading data from the main memory to a register, the CPU looks up the L1 cache first. If the CPU cannot find the data it is looking for in the L1 cache &#40;or cache miss&#41;, it checks the L2 cache and L3 cache in order. Once the desired data is found in the main memory, not only the required data, but also its physical neighbors are loaded to the caches. The wisdom behind the caching system is data locality, i.e. whenever a data is used, the data physically close to it has much higher probability to be used than the rest. Locality is particularly true when a program perform elementwise operations to an array with contiguous storage.</p> <p>When loading a 32/64-bit data from the memory to a register, the computer loads a chunk of data to the caches. The wisdom behind this design is the locallity of data using, i.e. a data is much more likely to be used if it is physically close to the data currently in use.</p> <p>GPUs hide the latency issues by launching a lot of threads at the same time. When one thread gets stuck and waiting, a GPU simply trys to find another thread that is about to execute. Without enough threads, a GPU will be latency bottlenecked. In Compute Unified Device Architecture &#40;CUDA&#41; programming, the number of CUDA cores determines the number of threads that can be simultaneously executed. CUDA cores are very similar to CPU cores, while the number of which can easily go up to several thousand, they are not as independent as CPU cores. The style of the execution of CUDA programs are called single instruction multiple threads &#40;SIMT&#41;, in which multiple data can be processed by a single instruction. While the style how CPU cores work is called multiple instruction multiple data &#40;MIMD&#41;, which does not require all threads to execute the same instruction.</p> <h2 id=hands_on_get_cpu_and_memory_information ><a href="#hands_on_get_cpu_and_memory_information" class=header-anchor >Hands on: Get CPU and memory information</a></h2> <h3 id=the_computing_power_of_a_cpu ><a href="#the_computing_power_of_a_cpu" class=header-anchor >The computing power of a CPU</a></h3> <p>In the linux system, the CPU information can be printed by typing <code>lscpu</code>.</p> <pre><code class="julia hljs">$ lscpu
Architecture:            x86_64
  CPU op-mode(s):        <span class=hljs-number >32</span>-bit, <span class=hljs-number >64</span>-bit
  Address sizes:         <span class=hljs-number >39</span> bits physical, <span class=hljs-number >48</span> bits virtual
  Byte Order:            Little Endian
CPU(s):                  <span class=hljs-number >8</span>
  On-line CPU(s) list:   <span class=hljs-number >0</span>-<span class=hljs-number >7</span>
Vendor ID:               GenuineIntel
  Model name:            Intel(R) Core(TM) i7-<span class=hljs-number >10510</span>U CPU @ <span class=hljs-number >1.80</span>GHz
    CPU family:          <span class=hljs-number >6</span>
    Model:               <span class=hljs-number >142</span>
    Thread(s) per core:  <span class=hljs-number >2</span>
    Core(s) per socket:  <span class=hljs-number >4</span>
    Socket(s):           <span class=hljs-number >1</span>
    Stepping:            <span class=hljs-number >12</span>
    CPU max MHz:         <span class=hljs-number >4900.0000</span>
    CPU min MHz:         <span class=hljs-number >400.0000</span>
    BogoMIPS:            <span class=hljs-number >4599.93</span>
    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc
                         a cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss 
                         ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art
                          arch_perfmon pebs bts rep_good nopl xtopology nonstop_
                         tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cp
                         l vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1
                          sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsav
                         e avx f16c rdrand lahf_lm abm <span class=hljs-number >3</span>dnowprefetch cpuid_fault
                          epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced 
                         tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase t
                         sc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid mpx rdse
                         ed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1
                          xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_
                         window hwp_epp md_clear flush_l1d arch_capabilities
Virtualization features: 
  Virtualization:        VT-x
Caches (sum of all):     
  L1d:                   <span class=hljs-number >128</span> KiB (<span class=hljs-number >4</span> instances)
  L1i:                   <span class=hljs-number >128</span> KiB (<span class=hljs-number >4</span> instances)
  L2:                    <span class=hljs-number >1</span> MiB (<span class=hljs-number >4</span> instances)
  L3:                    <span class=hljs-number >8</span> MiB (<span class=hljs-number >1</span> instance)
NUMA:                    
  NUMA node(s):          <span class=hljs-number >1</span>
  NUMA node0 CPU(s):     <span class=hljs-number >0</span>-<span class=hljs-number >7</span>
Vulnerabilities:         
  Itlb multihit:         KVM: Mitigation: VMX disabled
  L1tf:                  Not affected
  Mds:                   Not affected
  Meltdown:              Not affected
  Mmio stale data:       Mitigation; Clear CPU buffers; SMT vulnerable
  Retbleed:              Mitigation; Enhanced IBRS
  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl
                          and seccomp
  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer
                          sanitization
  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB fillin
                         g, PBRSB-eIBRS SW sequence
  Srbds:                 Mitigation; Microcode
  Tsx async abort:       Not affected</code></pre> <p>From the output of <code>lscpu</code>, one can see the L1d and L1i are the L1 caches for data and instructions, both having a size of 128 KB. The L2 and L3 caches are larger, they have size 1 MB and 8 MB respectively. Depending on the different energy strategy and the payload, the CPU clock rate can vary between 0.4 GHz and 4.9 GHz. The flags tell the information about the instruction sets, among which the <code>avx2</code> instruction set is for the single instruction multiple data &#40;SIMD&#41; parallelism. It uses a 256 wide register to pack 4 double precision floating point numbers or 8 single precision floating point numbers, and perform multiply and add operation on them in a single CPU clock cycle.</p> <p>A CPU may have multiple cores, and its theoretical upper bound of computing power can be measured by the number of floating point operations your computing device and perform in one second, namely, in floating point operations per second &#40;FLOPS&#41;. For the above CPU, the computing power can be estimated with</p> <pre><code class="julia hljs">Double precision computing power of a CPU = <span class=hljs-number >4.9</span> (CPU clock speed)
            * <span class=hljs-number >2</span> (due to the fused multiply-add (<span class=hljs-string >`fma`</span>) instruction)
            * <span class=hljs-number >4</span> (avx instruction set has a <span class=hljs-number >256</span> with register, it can
                crunch <span class=hljs-number >4</span> vectorized double precision floating point
                operations at one CPU cycle)
            * <span class=hljs-number >4</span> (number of cores)
            * sometimes extra factor
        = ? GFLOPS</code></pre> <p>An easy way to check the single/double precision FLOPS is using the matrix multiplication. Try the following code in a Julia REPL</p> <pre><code class="julia hljs">julia&gt; <span class=hljs-keyword >using</span> LinearAlgebra

julia&gt; LinearAlgebra.versioninfo()
BLAS: libblastrampoline.so (f2c_capable)
  --&gt; /home/leo/.julia/juliaup/julia-<span class=hljs-number >1.9</span><span class=hljs-number >.0</span>-beta2+<span class=hljs-number >0.</span>x64.linux.gnu/bin/../lib/julia/libopenblas64_.so (ILP64)
Threading:
  Threads.threadpoolsize() = <span class=hljs-number >1</span>
  Threads.maxthreadid() = <span class=hljs-number >1</span>
  LinearAlgebra.BLAS.get_num_threads() = <span class=hljs-number >1</span>
Relevant environment variables:
  OPENBLAS_NUM_THREADS = <span class=hljs-number >1</span>

julia&gt; n = <span class=hljs-number >1000</span>
<span class=hljs-number >1000</span>

julia&gt; A, B, C = randn(<span class=hljs-built_in >Float64</span>, n, n), randn(<span class=hljs-built_in >Float64</span>, n, n), zeros(<span class=hljs-built_in >Float64</span>, n, n);

julia&gt; BLAS.set_num_threads(<span class=hljs-number >1</span>)
julia&gt; <span class=hljs-meta >@benchmark</span> mul!($C, $A, $B)
BenchmarkTools.Trial: <span class=hljs-number >108</span> samples with <span class=hljs-number >1</span> evaluation.
 Range (min … max):  <span class=hljs-number >44.629</span> ms … <span class=hljs-number >55.949</span> ms  ┊ GC (min … max): <span class=hljs-number >0.00</span>% … <span class=hljs-number >0.00</span>%
 Time  (median):     <span class=hljs-number >46.178</span> ms              ┊ GC (median):    <span class=hljs-number >0.00</span>%
 Time  (mean ± σ):   <span class=hljs-number >46.571</span> ms ±  <span class=hljs-number >1.634</span> ms  ┊ GC (mean ± σ):  <span class=hljs-number >0.00</span>% ± <span class=hljs-number >0.00</span>%

        ▁▁█▅▁ ▁
  ▃▃▆▃▆██████▇█▇▇▅▅▃▁▃▄▁▁▃▃▁▄▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▃ ▃
  <span class=hljs-number >44.6</span> ms         Histogram: frequency by time        <span class=hljs-number >54.1</span> ms &lt;

 Memory estimate: <span class=hljs-number >0</span> bytes, allocs estimate: <span class=hljs-number >0.</span>

julia&gt; BLAS.set_num_threads(<span class=hljs-number >4</span>)

julia&gt; <span class=hljs-meta >@benchmark</span> mul!($C, $A, $B)
BenchmarkTools.Trial: <span class=hljs-number >299</span> samples with <span class=hljs-number >1</span> evaluation.
 Range (min … max):  <span class=hljs-number >15.252</span> ms … <span class=hljs-number >26.505</span> ms  ┊ GC (min … max): <span class=hljs-number >0.00</span>% … <span class=hljs-number >0.00</span>%
 Time  (median):     <span class=hljs-number >16.303</span> ms              ┊ GC (median):    <span class=hljs-number >0.00</span>%
 Time  (mean ± σ):   <span class=hljs-number >16.716</span> ms ±  <span class=hljs-number >1.831</span> ms  ┊ GC (mean ± σ):  <span class=hljs-number >0.00</span>% ± <span class=hljs-number >0.00</span>%</code></pre> <p>It can be shown that the computing power of a single thread is at least <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><msup><mn>0</mn><mn>3</mn></msup><mo>×</mo><mn>2</mn><mi mathvariant=normal >/</mi><mn>0.044629</mn><mo>≈</mo><mn>44.8</mn><mrow><mi mathvariant=normal >G</mi><mi mathvariant=normal >F</mi><mi mathvariant=normal >L</mi><mi mathvariant=normal >O</mi><mi mathvariant=normal >P</mi><mi mathvariant=normal >S</mi></mrow></mrow><annotation encoding="application/x-tex">1000^3 \times 2 / 0.044629 \approx 44.8 {\rm GFLOPS}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8974em;vertical-align:-0.0833em;"></span><span class=mord >100</span><span class=mord ><span class=mord >0</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >×</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mord >2/0.044629</span><span class=mspace  style="margin-right:0.2778em;"></span><span class=mrel >≈</span><span class=mspace  style="margin-right:0.2778em;"></span></span><span class=base ><span class=strut  style="height:0.6833em;"></span><span class=mord >44.8</span><span class=mord ><span class=mord ><span class="mord mathrm">GFLOPS</span></span></span></span></span></span>. The computing time of a carefully implemented matrix multiplication function is dominated by the <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><msup><mn>0</mn><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">1000^3</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8141em;"></span><span class=mord >100</span><span class=mord ><span class=mord >0</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span></span></span></span> multiplication operations and the same amount of addition operations. Dividing <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1000</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">1000 \times 2</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.7278em;vertical-align:-0.0833em;"></span><span class=mord >1000</span><span class=mspace  style="margin-right:0.2222em;"></span><span class=mbin >×</span><span class=mspace  style="margin-right:0.2222em;"></span></span><span class=base ><span class=strut  style="height:0.6444em;"></span><span class=mord >2</span></span></span></span> floating point operations in total by the amount of time in the the unit of seconds is the peak FLOPS. Here we have used the minimum computing time because the system does not always allocate time slices to the program we want to benchmark, using the minimum time gives us the best approximate to the time we want to measure.</p> <p>For the parallel execution, although we have 4 CPU cores, the 4-thread computation does not give us a 4 time speed up. This may have different causes, the cores may share caches, the operating system may not allocate all resources to this specific computing task.</p> <p>In addition, since we are using intel CPU, MKL is slightly faster than the default OpenBLAS. If we repeat the above numerical experiment, we will get the following timing information, which measures the peak performance of a CPU more acurately.</p> <pre><code class="julia hljs">julia&gt; <span class=hljs-keyword >using</span> MKL

julia&gt; LinearAlgebra.versioninfo()
BLAS: libblastrampoline.so (f2c_capable)
  --&gt; /home/leo/.julia/artifacts/<span class=hljs-number >347e4</span>bf25d69805922225ce6bf819ef0b8715426/lib/libmkl_rt.so (ILP64)
  --&gt; /home/leo/.julia/artifacts/<span class=hljs-number >347e4</span>bf25d69805922225ce6bf819ef0b8715426/lib/libmkl_rt.so (LP64)
Threading:
  Threads.threadpoolsize() = <span class=hljs-number >1</span>
  Threads.maxthreadid() = <span class=hljs-number >1</span>
  LinearAlgebra.BLAS.get_num_threads() = <span class=hljs-number >4</span>
Relevant environment variables:
  OPENBLAS_NUM_THREADS = <span class=hljs-number >1</span>

julia&gt; BLAS.set_num_threads(<span class=hljs-number >1</span>)

julia&gt; <span class=hljs-meta >@benchmark</span> mul!($C, $A, $B)
BenchmarkTools.Trial: <span class=hljs-number >113</span> samples with <span class=hljs-number >1</span> evaluation.
 Range (min … max):  <span class=hljs-number >42.150</span> ms … <span class=hljs-number >50.687</span> ms  ┊ GC (min … max): <span class=hljs-number >0.00</span>% … <span class=hljs-number >0.00</span>%
 Time  (median):     <span class=hljs-number >44.179</span> ms              ┊ GC (median):    <span class=hljs-number >0.00</span>%
 Time  (mean ± σ):   <span class=hljs-number >44.434</span> ms ±  <span class=hljs-number >1.435</span> ms  ┊ GC (mean ± σ):  <span class=hljs-number >0.00</span>% ± <span class=hljs-number >0.00</span>%

         ▅▅▂▂▂▅▂▂▅ ▂▃ █                                        
  ▄▄▁▇▇▄▇████████████▅█▇▄█▄▁▄▇▁▁▅▁▁▁▁▄▄▁▁▄▁▄▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▄ ▄
  <span class=hljs-number >42.1</span> ms         Histogram: frequency by time        <span class=hljs-number >50.4</span> ms &lt;

 Memory estimate: <span class=hljs-number >0</span> bytes, allocs estimate: <span class=hljs-number >0.</span>

julia&gt; BLAS.set_num_threads(<span class=hljs-number >4</span>)

julia&gt; <span class=hljs-meta >@benchmark</span> mul!($C, $A, $B)
BenchmarkTools.Trial: <span class=hljs-number >281</span> samples with <span class=hljs-number >1</span> evaluation.
 Range (min … max):  <span class=hljs-number >14.882</span> ms … <span class=hljs-number >26.696</span> ms  ┊ GC (min … max): <span class=hljs-number >0.00</span>% … <span class=hljs-number >0.00</span>%
 Time  (median):     <span class=hljs-number >15.791</span> ms              ┊ GC (median):    <span class=hljs-number >0.00</span>%
 Time  (mean ± σ):   <span class=hljs-number >17.775</span> ms ±  <span class=hljs-number >3.766</span> ms  ┊ GC (mean ± σ):  <span class=hljs-number >0.00</span>% ± <span class=hljs-number >0.00</span>%

    ▃██▄                                                       
  ▄▇█████▄▃▃▂▁▁▁▃▃▂▂▂▁▂▁▃▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▃▄▆▆▅▃▃ ▃
  <span class=hljs-number >14.9</span> ms         Histogram: frequency by time        <span class=hljs-number >25.4</span> ms &lt;

 Memory estimate: <span class=hljs-number >0</span> bytes, allocs estimate: <span class=hljs-number >0.</span></code></pre> <pre><code class="bash hljs">$ lsmem
RANGE                                  SIZE  STATE REMOVABLE  BLOCK
0x0000000000000000-0x000000007fffffff    2G online       <span class=hljs-built_in >yes</span>   0-15
0x0000000088000000-0x000000008fffffff  128M online       <span class=hljs-built_in >yes</span>     17
0x0000000100000000-0x0000000a6fffffff 37.8G online       <span class=hljs-built_in >yes</span> 32-333

Memory block size:       128M
Total online memory:    39.9G
Total offline memory:      0B</code></pre> <h3 id=the_computing_power_of_a_gpu ><a href="#the_computing_power_of_a_gpu" class=header-anchor >The computing power of a GPU</a></h3> <p>You may check your GPU information with the following command,</p> <pre><code class="bash hljs">$ nvidia-smi
Tue Feb 14 20:55:02 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| 30%   28C    P8     7W /  75W |    815MiB /  4096MiB |      4%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+</code></pre> <p>CUDA peak FLOPS is much easier to estimate, which is</p> <pre><code class="julia hljs">Double precision computing power of a GPU = the GPU frequency
            * the number of CUDA cores
            * <span class=hljs-number >2</span> (the <span class=hljs-string >`fma`</span> support)</code></pre> <p>One can verify the result with the official data sheet of a CUDA GPU or benchmark the CUDA performance with julia.</p> <pre><code class="julia hljs">julia&gt; <span class=hljs-keyword >using</span> BenchmarkTools, CUDA

julia&gt; <span class=hljs-keyword >using</span> LinearAlgebra: mul!

julia&gt; A, B = CUDA.randn(<span class=hljs-number >1000</span>, <span class=hljs-number >1000</span>), CUDA.randn(<span class=hljs-number >1000</span>, <span class=hljs-number >1000</span>);

julia&gt; C = CUDA.zeros(<span class=hljs-number >1000</span>, <span class=hljs-number >1000</span>);

julia&gt; <span class=hljs-meta >@benchmark</span> CUDA.<span class=hljs-meta >@sync</span> mul!($C, $A, $B)
BenchmarkTools.Trial: <span class=hljs-number >4206</span> samples with <span class=hljs-number >1</span> evaluation.
 Range (min … max):  <span class=hljs-number >814.601</span> μs …  <span class=hljs-number >13.147</span> ms  ┊ GC (min … max): <span class=hljs-number >0.00</span>% … <span class=hljs-number >0.00</span>%
 Time  (median):       <span class=hljs-number >1.182</span> ms               ┊ GC (median):    <span class=hljs-number >0.00</span>%
 Time  (mean ± σ):     <span class=hljs-number >1.172</span> ms ± <span class=hljs-number >264.659</span> μs  ┊ GC (mean ± σ):  <span class=hljs-number >0.00</span>% ± <span class=hljs-number >0.00</span>%

                    █▃               ▁▁          ▁▁              
  ▂▂▁▂▂▂▂▃▃▃▄▆▄▅▅▅▆▆██▅▆▄▅▄▄▄▅▅▄▆▆▆▆▆██▇▆▅▆▅▇▇▇▆▇███▆▆▅▄▃▃▅▄▃▃▂ ▄
  <span class=hljs-number >815</span> μs           Histogram: frequency by time         <span class=hljs-number >1.45</span> ms &lt;

 Memory estimate: <span class=hljs-number >416</span> bytes, allocs estimate: <span class=hljs-number >24.</span></code></pre> <h2 id=references ><a href="#references" class=header-anchor >References</a></h2> <ol> <li><p><a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf">https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf</a></p> </ol> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Jinguo Liu. Last modified: May 17, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> <script src="/ModernScientificComputing/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>